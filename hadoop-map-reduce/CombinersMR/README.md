# MapReduce - Combiner Program

* Combiner is a semi-reducer in mapreduce. This is an optional class which can be specified in mapreduce driver class to process the output of map tasks before submitting it to reducer tasks.

* In Mapreduce framework, usually the output from the map tasks is large and data transfer between map and reduce tasks will be high. Since the data transfer across the network is expensive and to limit the volume of data transfer between map and reduce tasks.

* Combiner functions summarize the map output records with the same key and output of combiner will be sent over network to actual reduce task as input.

* The combiner does not have its own interface and it must implement Reducer interface and reduce() method of combiner will be called on each map output key. The combiner class’s reduce() method must have the same input and output key-value types as the reducer class.

* Combiner functions are suitable for producing summary information from a large data set because combiner will replace that set of original map outputs, ideally with fewer records or smaller records.

* Hadoop doesn’t guarantee on how many times a combiner function will be called for each map output key. At times, it may not be executed at all, while at times it may be used once, twice, or more times depending on the size and number of output files generated by the mapper for each reducer.
 
* It is a general practice that, the same reducer class is used a combiner class many times. but this practice leads to some undesired results in some cases. The combiner function must only aggregate values and It is very important that the combiner class not have side effects, and that the actual reducer be able to properly process the results of the combiner.

## Prerequisite
Apached Hadoop 2.7.1  
Apache Maven 3.3.9  
Java version: 1.8.0_101, vendor: Oracle Corporation  
Default locale: en_SG, platform encoding: Cp1252  
OS name: "windows 10", version: "10.0", arch: "amd64", family: "dos"  
Eclipse Java EE IDE for Web Developers. Version: Mars.2 Release (4.5.2)  

#MapReduce Combiner Implementation
The following example provides a theoretical idea about combiners. Let us assume we have the following input text file named wordcount.txt for MapReduce.
```bash
What do you mean by Hadoop
What do you know about spark
What is Hadoop File System
How Hadoop executes mapreduce program
```

##Record Reader
This is the first phase of MapReduce where the Record Reader reads every line from the input text file as text and yields output as key-value pairs.

Input − Line by line text from the input file.

Output − Forms the key-value pairs. The following is the set of expected key-value pairs.
```bash
(1,What do you mean by Hadoop)
(2,What do you know about spark)
(3,What is Hadoop File System)
(4,How Hadoop executes mapreduce program)
```
##Map Phase
The Map phase takes input from the Record Reader, processes it, and produces the output as another set of key-value pairs.

Input − The following key-value pair is the input taken from the Record Reader.
```bash
(1,What do you mean by Hadoop)
(2,What do you know about spark)
(3,What is Hadoop File System)
(4,How Hadoop executes mapreduce program)
```
The Map phase reads each key-value pair, divides each word from the value using StringTokenizer, treats each word as key and the count of that word as value.

Output − The expected output is as follows −
```bash
(What,1) (do,1) (you,1) (mean,1) (by,1) (Hadoop,1)
(What,1) (do,1) (you,1) (know,1) (about,1) (spark,1)
(What,1) (is,1) (Hadoop,1) (File,1) (System,1)
(How,1) (Hadoop,1) (executes,1) (mapreduce,1) (program,1)
```

#Combiner Phase
The Combiner phase takes each key-value pair from the Map phase, processes it, and produces the output as key-value collection pairs.

##Input.  
```bash
(What,1) (do,1) (you,1) (mean,1) (by,1) (Hadoop,1)
(What,1) (do,1) (you,1) (know,1) (about,1) (spark,1)
(What,1) (is,1) (Hadoop,1) (File,1) (System,1)
(How,1) (Hadoop,1) (executes,1) (mapreduce,1) (program,1)
```
The Combiner phase reads each key-value pair, combines the common words as key and values as collection. Usually, the code and operation for a Combiner is similar to that of a Reducer.

##Output
```bash
(What,1,1,1) (do,1,1) (you,1,1) (mean,1) (by,1) (Hadoop,1,1,1)
(know,1) (about,1) (spark,1) 
(is,1) (File,1) (System,1)
(How,1) (executes,1) (mapreduce,1) (program,1)
```
#Reducer Phase
The Reducer phase takes each key-value collection pair from the Combiner phase, processes it, and passes the output as key-value pairs. Note that the Combiner functionality is same as the Reducer.

##Input
```bash
(What,1,1,1) (do,1,1) (you,1,1) (mean,1) (by,1) (Hadoop,1,1,1)
(know,1) (about,1) (spark,1) 
(is,1) (File,1) (System,1)
(How,1) (executes,1) (mapreduce,1) (program,1)
```

##Output
```bash
(What,2) (do,2) (you,2) (mean,1) (by,1) (Hadoop,3)
(know,1) (about,1) (spark,1) 
(is,1) (File,1) (System,1)
(How,1) (executes,1) (mapreduce,1) (program,1)
```
#Record Writer
This is the last phase of MapReduce where the Record Writer writes every key-value pair from the Reducer phase and sends the output as text.

##Input 
```bash
(What,2) (do,2) (you,2) (mean,1) (by,1) (Hadoop,3)
(know,1) (about,1) (spark,1) 
(is,1) (File,1) (System,1)
(How,1) (executes,1) (mapreduce,1) (program,1)
```
 
##Output
```bash
What		2 
do			2 
you 		2 
mean		1 
by			1 
Hadoop 		3
know 		1 
about		1 
spark		1 
is			1 
File		1 
System		1
How			1 
executes	1 
mapreduce	1 
program 	1
```

Sample input/result files are provided in the project under resources/input, resources/output
